## ABSTRACT
There is a huge development in the field of speech synthesis that is not only close to human naturalness, but is also capable of instant voice cloning with little data, and is highly accessible with pre-trained models available. Naturally, the potential flood of generated content raises the need for synthetic speech detection and watermarking. We propose a collaborative training scheme that embeds specific watermarks into speech synthesized by different users and enables the detection of these watermarks , but remains transparent to a human listener. Specifically, we employs a straightforward and effective GAN based on HiFi-GAN to restructure audio and embed a watermark, then trains a detection model to detect the type of watermark in the audio. Finally, listening tests demonstrate that collaborative training has little adverse effect on perceptual quality of vocoded speech. 

## INTRODUCTION
Modern speech synthesis systems have achieved nearly human level naturalness and are capable of zero-shot voice cloning from a few seconds of adaptation data. Open-source implementations, sharing pre-trained models, and good software packaging has made voice cloning with TTS easily accessible also outside the research community. Furthermore, the use of voice cloning technology as a service has recently emerged as a popular past-time on the Internet. With this growing user base, the amount of generated speech in the wild is increasing, which poses a risk of casually created misinformation and malicious deepfakes. 

Research on audio deepfake detection focuses mostly on passive protection using machine learning methods, which is also referred to as speech anti-spoofing. This scenario assumes that defenders have no prior knowledge of what attackers will use to generate the audio deepfake. The attackers can use any speech generation models to create the audio deepfake, while the defenders only have a limited number of audio deepfake types in the training set, which does not necessarily cover those from the attackers. Hence, the key question for the defender is how to develop a detection model on the basis of the limited training data and make it generalize to unseen deepfakes. 

Research outcomes from, for example, the ASVspoof and Audio Deep synthesis Detection (ADD) challenges, have demonstrated some deep learning-based detectors can detect certain unseen deepfakes in the benchmark datasets with error rates smaller than 5%. However, many detectors were found to be vulnerable to spurious features in the training set and generalize poorly to data from different domains. Even though the generalization capability may be improved by including more diverse training data, the long-term equilibrium of this adversarial game of passive audio deepfake detection is yet to be seen. Attackers can always create audio deepfake from newer and stronger speech generation models. 

Generative adversarial networks (GANs) take a mirrored perspective on playing the adversarial game of detection and generation. In this setting, a discriminator network attempts to classify between samples from the real and generated data distributions, while another network, the Generator synthesizes new samples and tries to spoof the discriminator into classifying the generated samples as real. The theoretical equilibrium for the GAN game is that the generator learns to match the real data distribution exactly, and discriminating between real and generated samples becomes impossible. Despite practical challenges with finite model capacity, numerics, and unstable training dynamics, GAN-based synthesis methods can be used for realistic speech waveform synthesis. In particular, HiFi-GAN is widely used as a high-quality neural vocoder in text-to-speech synthesis and voice-conversion. 

The adversarial perspective remains highly relevant in defence against black-hat attack scenarios. However, the intended use of a generative model is often not malicious, and deceptive realism is merely a side-product of a high-quality system. In these use cases, the generative model provided is likely happy to comply to any regulation and make an active effort to embed a watermark  into their generated model outputs as such. Moreover, a unique watermark could even be embedded  into audio for each user, allowing identification of user by simply detecting the watermark type from the restructured audio. The relevance of watermarking is amplified by the common use of pre-trained models and hosted services: if a model has built-in watermarking that is not trivial to remove and does not affect the perceptual quality of the output, most users will not make an effort to remove the watermark. 

This paper proposes a collaborative training scheme that tasks a generative model to watermark its output in specific ways for different users to be more easily detectable by a specific classifier without degrading the perceptual quality. The experiments use HiFi-GAN as the generative model and ASVspoof 2021 challenge baseline countermeasure models as watermark detector models. Additionally, the detection performance under additive noise and time-stretching is shown to improve by differentiable augmentation during training. Results show that collaborative training consistently improves the detection performance compared to the corresponding passively trained countermeasure model.
