## ABSTRACT
It remains a challenge to effectively control the style rendering in text-to-speech (TTS) synthesis. Prior studies have primarily utilized an utterance-level style embedding extracted from reference audio and neglect the inherent multi-periodic property of speech style. This paper introduces a novel non-autoregressive framework that model styles as a latent random variable to generate the most suitable style for the text without requiring reference speech. Firstly,  We propose a multi-periodic style feature extractor to captures the latent style features of different periodic signals in audio. Secondly,  a novel architecture with the multi-stage style module is specially designed to model the pronunciation and high-level style expressiveness respectively with the latent style features and linguistic features from a acoustic text encoderã€‚Our proposed approach yields improved performance in subjective evaluations, demonstrating the ability to generate cross-lingual speech.
## INTRODUCTION

Speech synthesis, also known as Text-to-Speech (TTS), is a field that focuses on generating natural and intelligible speech from textual input. It plays a crucial role in various applications, such as voice assistants, audiobooks, and accessibility tools. With the advancements in deep learning, significant strides have been made in the field of speech synthesis. Presently, the majority of state-of-the-art neural speech synthesis systems employ a two-stage pipeline including an acoustic model and a vocoder. In a two-stage speech synthesis system, the acoustic model serves as the first stage, transforming textual information into Mel-spectrograms. Subsequently, the vocoder converts the generated Mel-spectrograms into speech waveforms. It is worth noting that the quality of the synthesized speech mainly relies on the acoustic features produced by the acoustic models. 

The typical speech synthesis models, such as Tacotron, Transformer-TTS, FastSpeech, DurIAN, have achieved tremendous success. But there is still room for improvement of the acoustic model. The diffusion models including the denoising diffusion probabilistic models (DDPM) and score-based generative models have attracted much attention due to its potential to generate high-quality samples. However, a notable drawback of diffusion models is its reliance on many iterations to generate satisfactory samples. And several  methods have been proposed for speech synthesis utilizing diffusion models. Nonetheless, a prevalent challenge encountered by these methods is the issue of slow generation speed. 

Diff-TTS leverages a DDPM framework to convert a noise signal into a Mel-spectrogram through multiple diffusion time steps. DiffSpeech introduces a shallow diffusion mechanism to enhance voice quality and accelerate inference speed. Grad-TTS formulates a stochastic differential equation (SDE) to gradually transform noise into a mel-spectrogram, employing a numerical ODE solver to solve the reverse SDE. Although it produces high-quality audio, the inference speed is slowed down due to the large number of iterations in the reverse process. ProDiff further develops the approach by utilizing progressive distillation to reduce the number of sampling steps. DiffGAN-TTS adopts an adversarially trained model to approximate the denoising function, enabling efficient speech synthesis. ResGrad employs the diffusion model to estimate the prediction residual from a pre-trained FastSpeech2 model and ground truth. CoMoSpeech achieves one-step high quality speech synthesis by utilizing a consistency model [16], which is distilled from a pre-trained teacher model. A recent work Voice box [17] is proposed to generate masked speech given its surrounding audio and text transcript on a text-guided speech infilling task, which is a non-autoregressive continuous normalizing flow (CNF) model trained with flow-matching method.

Recent advancements in speech synthesis systems have enabled the generation of high-quality speech. However, in some complex scenarios, such as human-computer interaction (HCI), these systems still fall short since they are unable to generate audio with natural and human-like prosody. At present, there are two mainstream approaches to model the speaking style information: one uses pre-defined categorical style labels as the global control condition of TTS systems to denote different speaking styles and the other imitates the speaking style given a reference speech. For the first kind of approach, the style control strategy is more in tuitive and interpretable, which is more suitable for practical TTS applications. For the second one, the global style tokens or style embeddings extracted from the training datasets can enrich the diversity of expressiveness and additional style labels are not required.

To effectively interpret the latent space of style feature and generate speech from an unseen style feature during inference, our systems require substantial training data encompassing a diverse set of style labels. However, acquiring high-quality speech-text paired data for training is a costly and time-consuming endeavor. To overcome the data scarcity challenge, recent TTS systems have utilized crowd-sourced speech data or employed data augmentation techniques such as pitch shifting and synthesizing new speech using voice conversion or TTS systems. Nevertheless, these data sources often contain speech with ambiguous pronunciation, background noise, channel artifacts, and artificial distor tions, which result in degradation of the overall performance of the TTS systems. 

To address these issues, we propose an innovative TTS model that builds to present the next step towards human-level TTS systems. We model speech styles as a latent random variable and sample them with a multi-periodic style feature extractor, allowing the model to efficiently synthesize highly realistic speech without the need for reference audio. Flow matching is a straight forward yet effective method that augments the latent vectors of neural network systems, enhancing their robustness and generalization without reference speech. Subsequently, a multi-stage module is proposed as the feature fusion to predict acoustic features with linguistic hidden features and latent style features.
## PROPOSED METHOD
For the model structure of our TTS, it consists of acoustic text encoder, style text encoder, stochastic duration predictor,  normalizing flows, multi period style feature extractor, feature fusion, decoder and discriminator. The architecture of TTS is illustrated as Figure. Specifically, the acoustic text encoder is responsible for encoding the input text into linguistic hidden features. The multi-periodic style feature extractor is employed to captures the latent style features of different periodic signals in audio. Augmented with  normalizing flows, variational inference  improves expressive power of the style hidden features, while the stochastic duration predictor is utilized to expand the style hidden features to match the length of the corresponding latent style features. The feature fusion module fuses the linguistic hidden features and the latent style features into latent acoustic features, which are transformed into waveforms through the decoder based on HiFi-GAN. What's more, an adversarial training process is adopted improves the expressive power of generative modeling

In the following subsections, we describe detailed designs of the style text encoder, multi period style feature extractor and feature fusion module in our method.

### multi period style feature extractor

To uncover latent style features related to speech styles, we employ a multi period style feature extractor based on the Multi-Period Discriminator(MPD) from HiFi-GAN and extract latent style features from the linear spectrogram of speech audio. The multi period style feature extractor takes linear spectrogram as input and outputs a fixed dimensional style features. This style features is a weighted combination of the learned style tokens, which will be used as the conditional features of the acoustic model.

The multi period style feature extractor is a mixture of sub-extractors, each of which only accepts equally spaced samples of an input linear spectrogram; the space is given as period p. The sub-extractors are designed to capture different implicit structures from each other by looking at different parts of an input linear spectrogram. We set the periods to [2, 3, 5, 7, 11] to avoid overlaps as much as possible. 

We fold the 2D linear spectrogram into 3D according to the period p on the time scale and then sequentially apply a non-causal WaveNet residual blocks, which containing layers of dilated convolutions with a gated activation unit and skip connection, to the reshaped data. Then we reshape the 3D data back into 2D data. At the end of each sub-discriminator, we restrict the kernel size in the width axis to be 1 to process the periodic samples independently. Subsequently, weight normalization is applied to the extractor. By reshaping the input linear spectrogram into 3D data, gradients from the extractor can be delivered to all time steps of the input linear spectrogram.

### style text encoder

The style text encoder aims to extract style representation from the input text. In previous works, large pre-trained models are widely used as the text encoders, such as CLIP and BERT, which leads to huge model parameters and expensive computation. In this paper, we proposed an efficient text encoder. The current EfficientSpeech employs a fastspeech2 like network structure, which achieves efficient and high quality speech synthesis. Inspired by the architecture of EfficientSpeech, our efficient text encoder is proposed, which consists of a phoneme encoder and an acoustic features extractor. The phoneme encoder extracts content features from the input phoneme obtained by g2p. The acoustic features extractor predicts the style feature from the content features. Instead of predicting the acoustic features in series, the acoustic features extractor generates them in parallel which contributes to faster inference. The content features and acoustic features are concatenated

### feature fusion

The feature fusion consists of two components: encoder and decoder. A linear layer is applied to take the style features from the style text encoder as input and then split the linear layer output into several parts which is as hidden layer vector for encoders at different stages . In addition to the hidden layer vector, the encoder  also processes the output vector from the previous encoder into a sequence of output vector as the input of the next encoder and hidden layer vector as the hidden input of the corresponding decoder. The decoder processes the hidden layer vector of the corresponding encoder and the output vector of the previous decoder. The features from the text encoder serve as the input vector of the first encoder and the first decoder. The first decoder takes the output vector of the last encoder as input, and the output vector of the last decoder is used as the output of the feature fusion module. As shown in Figure, the encoders and decoders share similar model structure but different model parameters, which consists of a 2-layer 1D-convolutional network with ReLU activation, each followed by the layer normalization and the dropout layer, and an extra linear layer to project the hidden states into the output sequence.

## EXPERIMENTAL AND RESULTS
###  3.1. Dataset and Evaluation Metric
The dataset used for training is LJSpeech that is made of 13,100 audio clips with corresponding text transcripts. The training of model uses 12,588 clips while 512 clips are reserved for testing. The phoneme sequence is generated by the open-source tool g2p which convert English grapheme to phoneme. The waveform is transformed into linear spectrogram with window and FFT lengths of 1,024, hop length of 256 and sampling rate of 22,050. The mel spectrogram has 80 channels. 

The TTS system evaluation focuses on the generated speech quality. Moreover the evaluation gives priority to the number of parameters, amount of computations as measured by floating point operations (FLOPS), and sampling speed. For Mean Opinion Score (MOS) estimation, we synthesized 20 sentences from the test split with each model. The assessors were asked to estimate the quality of synthesized speech on a nine-point Likert scale, the lowest and the highest scores being 1 point and 5 points with a step of 0.5 point. The number of parameters refers to the amount of memory used in inference phase. GFLOPs reflects the  number of floating point operations needed to complete an inference. GFLOPs increases with the text sequence length. In our experiment, GFLOPs is measured using 128 randomly sampled texts from the test split. The inputs to each model are same. Sampling speed is usually measured in terms of Real Time Factor(RTF). RTF is how many seconds it takes to generate one second of audio. 

 ###  3.2. Implementation Details
Text Encoder: The phoneme sequence x phoneis an embedding of the input phonemes, where N is the sequence length and d=128 is the embedding size. The phoneme encoder is made of 2 transformer blocks. Each block is made of adepth-wise separable convolution,a Self-Attention layer and a typical transformer FFN. In the FFN, we add an additional convolution layer and use the GeLU activation between two linear layers. Layer Normalization is applied after Self-Attention and FFN. Both Self-Attention and FFN use residual connection for fast convergence. The acoustic features extractor consists of 2blocks. Each block includes a Convolution layer,Layer Normalization and a ReLU activation.

Spectrogram VQ Model: In this study, we follow VQ GAN, adopting similar network architecture for the VQ-VAE encoder Evq,decoder G, and discriminator D. To preserve more time dimension information, we set a downsampling factor of 2 a long the time axis, and a downsampling factor of 20 a long the frequency axis.For the codebook Z,the dimension of each code vectornzis set as 128, and the codebook sizeK issetas128. The learning rate is fixed and determined as a product of a base learning rate,the number of GPUs used and the batchsize. 

Discretecon trastive diffusion model: We built a 12 layer8-head transformer with adimension of 128 forthed if fusion model. Each transformer block contains a full-context attention,a linear fusion layer to combine conditional features and afeed-for ward network block.For the default setting, we setthetotaltimestepsT=100.The current timestept is added into the network with adaptive layer normalizati on operator. For the diffusion forward process, we linearly increase Î³t and Î²t from 0 to 0.9 and 0.1, respectively.

### 3.3. Comparison with Baseline 
We compare the results with the baseline models which were evaluated based on official implementations. Note that we set the inference steps of Grad-TTS to 100(Grad TTS-100), keeping the same inference steps as our TTS system.

Table shows the MOS evaluation metric as evaluated by 10 participants with high English listening comprehension. The synthesized speech samples are from the test split. Our results have great competitiveness in terms of audio quality, which indicates that our approach models the speech features effectively.

Not only that,our model is also very outstanding in efficiency, it has fewer parameters and GFlOPs that are used at inference phase. The effect of the small number of parameters and GFLOPS is faster melspectrogram generation, reaching mRTF of 73.9 on a single NVIDIA 3090Ti GPU as shown in Table2. The speed is more evident on anIn tel CPU where the TTS reaches mRTF of 17.6 which is 44.0Ã— faster compared to Grad-TTS. For mel-spectrogram generation, Tacotron and Grad-TTS are unable to run with satisfactory mRTF on a single CPU
### 3.4. Ablation Study

we conduct ablation experiments to study the contribution of the TCLL in our full model. As shown in Table, by comparing the CMOS[21] of DCTTS(w/o TCLL) and DCTTS, it is shown that the TCLL has a positive effect on the quality of generated speech.