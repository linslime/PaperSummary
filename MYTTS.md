## ABSTRACT
It remains a challenge to effectively control the emotion rendering in text-to-speech (TTS) synthesis. Prior studies have primarily utilized an utterance-level style embedding extracted from reference audio and neglect the inherent multi-periodic property of speech style. This paper introduces a novel non-autoregressive framework that model styles as a latent random variable to generate the most suitable style for the text without requiring reference speech. Firstly,  We propose a multi-periodic style feature extractor to captures the latent style features of different periodic signals in audio. Secondly,  a novel architecture with the multi-stage style decoder is specially designed to model the pronunciation and high-level style expressiveness respectively。Our proposed approach yields improved performance in both objective and subjective evaluations, demonstrating the ability to generate cross-lingual speech.
## INTRODUCTION

Recent advancements in speech synthesis systems have enabled the generation of high-quality speech. However, in some complex scenarios, such as human-computer interaction (HCI), these systems still fall short since they are unable to generate audio with natural and human-like prosody. At present, there are two mainstream approaches to model the speaking style information: one uses pre-defined categorical style labels as the global control condition of TTS systems to denote different speaking styles and the other imitates the speaking style given a reference speech. For the first kind of approach, the style control strategy is more in tuitive and interpretable, which is more suitable for practical TTS applications. For the second one, the global style tokens or style embeddings extracted from the training datasets can enrich the diversity of expressiveness and additional style labels are not required.

In this paper, we introduce an innovative TTS model that builds to present the next step towards human-level TTS systems. We model speech styles as a latent random variable and sample them with a multi-periodic style feature extractor, allowing the model to efficiently synthesize highly realistic speech without the need for reference audio. 

To effectively interpret the latent space of style feature and generate speech from an unseen style feature during inference, our systems require substantial training data encompassing a diverse set of style labels. However, acquiring high-quality speech-text paired data for training is a costly and time-consuming endeavor. To overcome the data scarcity challenge, recent TTS systems have utilized crowd-sourced speech data or employed data augmentation techniques such as pitch shifting and synthesizing new speech using voice conversion or TTS systems. Nevertheless, these data sources often contain speech with ambiguous pronunciation, background noise, channel artifacts, and artificial distor tions, which result in degradation of the overall performance of the TTS systems. 

To address these issues, we propose a novel approach called latent filling to address these challenges. LF aims to fill the unexplored regions of the latent space of style features through latent space data augmentation. Unlike data augmentation techniques applied directly to input data, latent space data augmentation is a straight forward yet effective method that augments the latent vectors of neural network systems, enhancing their robustness and generalization. Then, a multi-stage predictor is proposed as the feature analyzer to predict acoustic features with text features and style features.
## PROPOSED METHOD
For the model structure of our TTS, it consists of acoustic text encoder, style text encoder, stochastic duration predictor,  normalizing flows, multi period style feature extractor, feature fusion, decoder and discriminator. The architecture of ReFlow-TTS is illustrated as Figure.  

Specifically, the acoustic text encoder is responsible for encoding the input text into linguistic hidden features. The stochastic duration predictor is utilized to expand the linguistic hidden features to match the length of the corresponding mel-spectrograms.

The step encoder converts the step t to a step embedding using the sinusoidal position embedding with 256 channels. 

For the rectified flow decoder, we adopt a similar architecture as in DiffWave. The decoder network comprises a stack of 20 residual blocks incorporating ConvlD, tanh, sigmoid and 1x1 convolutions with 256 channels.

### multi period style feature extractor

To uncover latent acoustic variations related to speech styles, we employ a multi period style feature extractor and extract latent style features from speech audio. The multi period style feature extractor takes linear spectrogram as input and outputs a fixed dimensional style features. This style features is a weighted combination of the learned style tokens, which will be used as the conditional features of the acoustic model.

The multi period style feature extractor is a mixture of sub-extractors, each of which only accepts equally spaced samples of an input linear spectrogram; the space is given as period p. The sub-extractors are designed to capture different implicit structures from each other by looking at different parts of an input linear spectrogram. We set the periods to [2, 3, 5, 7, 11] to avoid overlaps as much as possible. 

We first reshape 1D raw audio of length T into 2D data of height T p and width p and then apply 2D convolutions to the reshaped data. In every convolutional layer of MPD, we restrict the kernel size in the width axis to be 1 to process the periodic samples independently. Each sub-discriminator is a stack of strided convolutional layers with leaky rectified linear unit (ReLU) activation. Subsequently, weight normalization (Salimans and Kingma, 2016) is applied to MPD. By reshaping the input linear spectrogram into 2D data instead of sampling periodic signals of audio, gradients from MPD canbe delivered to all time steps of the input linear spectrogram.

### style text encoder

The style text encoder aims to extract style representation from the input text. In previous works, large pre-trained models are widely used as the text encoders, such as CLIP and BERT, which leads to huge model parameters and expensive computation. In this paper, we proposed an efficient text encoder. The current EfficientSpeech employs a fastspeech2 like network structure, which achieves efficient and high quality speech synthesis. Inspired by the architecture of EfficientSpeech, our efficient text encoder is proposed, which consists of a phoneme encoder and an acoustic features extractor. The phoneme encoder extracts content features from the input phoneme obtained by g2p. The acoustic features extractor predicts the style feature from the content features. Instead of predicting the acoustic features in series, the acoustic features extractor generates them in parallel which contributes to faster inference. The content features and acoustic features are concatenated

### feature fusion

The MSP is proposed based on FastSpeech, a Non AutoRegressive (NAR) acoustic model with explicit duration modeling. We will introduce the model architecture, and the corresponding loss function for training. 

Model Architecture: As shown in Fig, MSP is composed of two parts, encoder, and decoder. The text sequence t, i.e. phoneme sequence in our work, is first encoded to hid den vectors ht = Et(t), and then up-sampled by repetition according to duration sequence ˆ d = DP(ht). Notably, ˆ d is a non-negative integer sequence, where each integer means the number how many frames its corresponding phoneme lasts in x. hu is down-sampled in stages by strided convolutional layers to the corresponding stages h(1) d ,...,h(S) d . The decoder predicts P from the highest stage. First, h(S) d is fed to the decoder DS to obtain the predicted sequence ˜ p(S), which is then quantized by the corresponding codebook cS to p(S). The last hidden output sequence in the decoder and p(S) are up-sampled by repetition to concatenate with h(S−1) d as the input of the next decoder DS−1. The remaining sequences aregenerated recursively in the same way to form the predicted MSMCRP. In training, the ground truth MSMCR Z and ground-truth durations d are available in advance. Hence, we adopt teacher forcing to train MSP, i.e. replacing ˆ d with d, and inputting {z(S),...,z(2)} to decoders. 

## EXPERIMENTAL AND RESULTS
###  3.1. Dataset and Evaluation Metric
The dataset used for training is LJSpeech that is made of 13,100 audio clips with corresponding text transcripts. The training of model uses 12,588 clips while 512 clips are reserved for testing. The phoneme sequence is generated by the open-source tool g2p which convert English grapheme to phoneme. The waveform is transformed into linear spectrogram with window and FFT lengths of 1,024, hop length of 256 and sampling rate of 22,050. The mel spectrogram has 80 channels. 

The TTS system evaluation focuses on the generated speech quality. Moreover the evaluation gives priority to the number of parameters, amount of computations as measured by floating point operations (FLOPS), and sampling speed. For Mean Opinion Score (MOS) estimation, we synthesized 20 sentences from the test split with each model. The assessors were asked to estimate the quality of synthesized speech on a nine-point Likert scale, the lowest and the highest scores being 1 point and 5 points with a step of 0.5 point. The number of parameters refers to the amount of memory used in inference phase. GFLOPs reflects the  number of floating point operations needed to complete an inference. GFLOPs increases with the text sequence length. In our experiment, GFLOPs is measured using 128 randomly sampled texts from the test split. The inputs to each model are same. Sampling speed is usually measured in terms of Real Time Factor(RTF). RTF is how many seconds it takes to generate one second of audio. 

 ###  3.2. Implementation Details
Text Encoder: The phoneme sequence x phone ∈ RN× d is an embedding of the input phonemes, where N is the sequence length and d=128 is the embedding size. The phoneme encoder is made of 2 transformer blocks. Each block is made of adepth-wise separable convolution,a Self-Attention layer and a typical transformer FFN[19] . In the FFN, we add an additional convolution layer and use the GeLU[20] activation between two linear layers. Layer Normalization is applied after Self-Attention and FFN. Both Self-Attention and FFN use residual connection for fast convergence. The acoustic features extractor consists of 2blocks. Each block includes a Convolution layer,Layer Normalization and a ReLU activation. The prediction of Energy: ye,Pitch: yp and Duration: yd are generated respectively by three extractors with same architecture.

Spectrogram VQ Model: In this study, we follow VQ GAN, adopting similar network architecture for the VQ-VAE encoder Evq,decoder G, and discriminator D. To preserve more time dimension information, we set a downsampling factor of 2 a long the time axis, and a downsampling factor of 20 a long the frequency axis.For the codebook Z,the dimension of each code vectornzis set as 128, and the codebook sizeK issetas128. The learning rate is fixed and determined as a product of a base learning rate,the number of GPUs used and the batchsize. 

Discretecon trastive diffusion model: We built a 12 layer8-head transformer with adimension of 128 forthed if fusion model. Each transformer block contains a full-context attention,a linear fusion layer to combine conditional features and afeed-for ward network block.For the default setting, we setthetotaltimestepsT=100.The current timestept is added into the network with adaptive layer normalizati on operator. For the diffusion forward process, we linearly increase γt and βt from 0 to 0.9 and 0.1, respectively.

### 3.3. Comparison with Baseline 
We compare the results with the baseline models which were evaluated based on official implementations. Note that we set the inference steps of Grad-TTS to 100(Grad TTS-100), keeping the same inference steps as our TTS system.

Table1 shows the MOS evaluation metric as evaluated by 10 participants with high English listening comprehension. The synthesized speech samples are from the test split. Our results have great competitiveness in terms of audio quality, which indicates that our approach models the speech features effectively.

Not only that,our model is also very outstanding in efficiency, it has fewer parameters and GFlOPs that are used at inference phase. The effect of the small number of parameters and GFLOPS is faster melspectrogram generation, reaching mRTF of 73.9 on a single NVIDIA1080TiGPU as shown in Table2. The speed is more evident on anIn tel CPU where DCTTS reaches mRTF of 17.6 which is 44.0× faster compared to Grad-TTS. For mel-spectrogram generation,Tacotron and Grad-TTS are unable to run with satisfactory mRTF on a single CPU
