## ABSTRACT
It remains a challenge to effectively control the emotion rendering in text-to-speech (TTS) synthesis. Prior studies have primarily utilized an utterance-level style embedding extracted from reference audio, neglecting the inherent multi-periodic property of speech prosody. This paper introduces a novel non-autoregressive framework that model styles as a latent random variable to generate the most suitable style for the text without requiring reference speech, achieving efficient latent diffusion. Firstly,  We propose a multi-periodic style feature extractor to captures the latent features of different periodic signals in audio. Secondly,  a novel architecture with the multi-stage style decoder is specially designed to model the pronunciation and high-level style expressiveness respectivelyã€‚Our proposed approach yields improved performance in both objective and subjective evaluations, demonstrating the ability to generate cross-lingual speech.
## INTRODUCTION

 Recent advancements in speech synthesis systems [1, 2, 3, 4] have enabled the generation of high-quality speech. However, in some complex scenarios, such as human-computer interaction (HCI), these systems still fall short since they are unable to generate audio with natural and human-like prosody.

 At present, there are two mainstream approaches to model the speaking style information: one uses pre-defined categorical style labels as the global control condition of TTS systems to denote different speaking styles [1] and the other imitates the speaking style given a reference speech [2, 3]. For the first kind of approach, the style control strategy is more in tuitive and interpretable, which is more suitable for practical TTS applications. For the second one, the global style tokens or style embeddings extracted from the training datasets can enrich the diversity of expressiveness and additional style la bels are not required.
