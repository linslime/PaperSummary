## ABSTRACT
It remains a challenge to effectively control the emotion rendering in text-to-speech (TTS) synthesis. Prior studies have primarily utilized an utterance-level style embedding extracted from reference audio and neglect the inherent multi-periodic property of speech style. This paper introduces a novel non-autoregressive framework that model styles as a latent random variable to generate the most suitable style for the text without requiring reference speech. Firstly,  We propose a multi-periodic style feature extractor to captures the latent style features of different periodic signals in audio. Secondly,  a novel architecture with the multi-stage style decoder is specially designed to model the pronunciation and high-level style expressiveness respectivelyã€‚Our proposed approach yields improved performance in both objective and subjective evaluations, demonstrating the ability to generate cross-lingual speech.
## INTRODUCTION

Recent advancements in speech synthesis systems have enabled the generation of high-quality speech. However, in some complex scenarios, such as human-computer interaction (HCI), these systems still fall short since they are unable to generate audio with natural and human-like prosody.At present, there are two mainstream approaches to model the speaking style information: one uses pre-defined categorical style labels as the global control condition of TTS systems to denote different speaking styles and the other imitates the speaking style given a reference speech. For the first kind of approach, the style control strategy is more in tuitive and interpretable, which is more suitable for practical TTS applications. For the second one, the global style tokens or style embeddings extracted from the training datasets can enrich the diversity of expressiveness and additional style labels are not required.

In this paper, we introduce an innovative TTS model that builds to present the next step towards human-level TTS systems. We model speech styles as a latent random variable and sample them with a multi-periodic style feature extractor, allowing the model to efficiently synthesize highly realistic speech without the need for reference audio. 

To effectively interpret the latent space of style feature and generate speech from an unseen style feature during inference, our systems require substantial training data encompassing a diverse set of style labels. However, acquiring high-quality speech-text paired data for training is a costly and time-consuming endeavor. To overcome the data scarcity challenge, recent TTS systems have utilized crowd-sourced speech data or employed data augmentation techniques such as pitch shifting and synthesizing new speech using voice conversion or TTS systems. Nevertheless, these data sources often contain speech with ambiguous pronunciation, background noise, channel artifacts, and artificial distor tions, which result in degradation of the overall performance of the TTS systems. 

To address these issues, we propose a novel approach called latent filling to address these challenges. LF aims to fill the unexplored regions of the latent space of style features through latent space data augmentation. Unlike data augmentation techniques applied directly to input data, latent space data augmentation is a straight forward yet effective method that augments the latent vectors of neural network systems, enhancing their robustness and generalization. Then, a multi-stage predictor is proposed as the feature analyzer to predict acoustic features with text features and style features.
