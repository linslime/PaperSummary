## ABSTRACT
It remains a challenge to effectively control the emotion rendering in text-to-speech (TTS) synthesis. Prior studies have primarily utilized an utterance-level style embedding extracted from reference audio and neglect the inherent multi-periodic property of speech style. This paper introduces a novel non-autoregressive framework that model styles as a latent random variable to generate the most suitable style for the text without requiring reference speech. Firstly,  We propose a multi-periodic style feature extractor to captures the latent style features of different periodic signals in audio. Secondly,  a novel architecture with the multi-stage style decoder is specially designed to model the pronunciation and high-level style expressiveness respectively。Our proposed approach yields improved performance in both objective and subjective evaluations, demonstrating the ability to generate cross-lingual speech.
## INTRODUCTION

Recent advancements in speech synthesis systems have enabled the generation of high-quality speech. However, in some complex scenarios, such as human-computer interaction (HCI), these systems still fall short since they are unable to generate audio with natural and human-like prosody.At present, there are two mainstream approaches to model the speaking style information: one uses pre-defined categorical style labels as the global control condition of TTS systems to denote different speaking styles and the other imitates the speaking style given a reference speech. For the first kind of approach, the style control strategy is more in tuitive and interpretable, which is more suitable for practical TTS applications. For the second one, the global style tokens or style embeddings extracted from the training datasets can enrich the diversity of expressiveness and additional style labels are not required.

In this paper, we introduce an innovative TTS model that builds to present the next step towards human-level TTS systems. We model speech styles as a latent random variable and sample them with a multi-periodic style feature extractor, allowing the model to efficiently synthesize highly realistic speech without the need for reference audio. 

To effectively interpret the latent space of style feature and generate speech from an unseen style feature during inference, our systems require substantial training data encompassing a diverse set of style labels. However, acquiring high-quality speech-text paired data for training is a costly and time-consuming endeavor. To overcome the data scarcity challenge, recent TTS systems have utilized crowd-sourced speech data or employed data augmentation techniques such as pitch shifting and synthesizing new speech using voice conversion or TTS systems. Nevertheless, these data sources often contain speech with ambiguous pronunciation, background noise, channel artifacts, and artificial distor tions, which result in degradation of the overall performance of the TTS systems. 

To address these issues, we propose a novel approach called latent filling to address these challenges. LF aims to fill the unexplored regions of the latent space of style features through latent space data augmentation. Unlike data augmentation techniques applied directly to input data, latent space data augmentation is a straight forward yet effective method that augments the latent vectors of neural network systems, enhancing their robustness and generalization. Then, a multi-stage predictor is proposed as the feature analyzer to predict acoustic features with text features and style features.
## PROPOSED METHOD

Multi-Period Discriminator MPD is a mixture of sub-discriminators, each of which only accepts equally spaced samples of an input audio; the space is given as period p. The sub-discriminators are designed to capture different implicit structures from each other by looking at different parts of an input audio. We set the periods to [2, 3, 5, 7, 11] to avoid overlaps as much as possible. As shown in Figure 2b, we first reshape 1D raw audio of length T into 2D data of height T p and width p and then apply 2D convolutions to the reshaped data. In every convolutional layer of MPD, we restrict the kernel size in the width axis to be 1 to process the periodic samples independently. Each sub-discriminator is a stack of strided convolutional layers with leaky rectified linear unit (ReLU) activation. Subsequently, weight normalization (Salimans and Kingma, 2016) is applied to MPD. By reshaping the input audio into 2D data instead of sampling periodic signals of audio, gradients from MPDcanbe delivered to all time steps of the input audio.

Through the latent filling (LF) method, our aim is to fill the latent space of the speaker embeddings that the training dataset cannot ad equately express. We employ two intuitive latent space augmenta tion techniques for the LF: interpolation [18,19] and noise addition [18]. Illustrations of these methods are provided in Fig. 2. The interpolation method creates a completely new speaker embedding  in the speaker embedding space by using two different speaker em beddings, while the adding noise method generates a new speaker embedding that is relatively close to the existing one. In the previ ous work [18], the authors also adopted the extrapolation for latent space data augmentation. However, in our preliminary experiments, we observed that extrapolation is not particularly meaningful in our case. The complete LF process is detailed in Algorithm 1. First, for the speaker embedding si, we randomly select another speaker em bedding sj from the training dataset for the interpolation. Our pre liminary study indicates that ensuring sj has the same language in formation as si is crucial for achieving stable performance. We per form interpolation between si and sj with a probability of 1 − ϵ, using the formula λsi + (1 − λ)sj. Here, ϵ represents the probabil ity of adding noise, while λ denotes the interpolation rate. Similar to [19], we sample λ from the beta distribution Beta(β,β), where β is the beta distribution shape parameter. When interpolation is con ducted, Gaussian noise G ∼ N(0,σ2) with a standard deviation σ is added with a probability of ϵ. Conversely, when the interpolation is not performed, the Gaussian noise is always added. 2.2.1. Latent filling consistency loss Adopting latent space data augmentation for generation tasks has been challenging due to the inherent difficulty of obtaining target data corresponding to augmented latent vectors. Similarly, when the LF method is adopted for the speaker embedding, it is impossible to calculate the LRec because there is no corresponding ground truth speech containing the speaker information for the augmented speaker embedding ˜ s. To address this challenge, we propose a latent filling consis tency loss (LFCL), a modifiedversionofSCLtailoredforaugmented speaker embeddings. The LFCL can be computed as follows: where ˜ x is a generated acoustic feature corresponding to ˜ s. LFCL measures the closeness between the speaker embedding of ˜ x and ˜ s, and encourages ˜ x to have the same speaker characteristics as the input augmented speaker embedding ˜ s. By using the LFCL, we can successfully update the ZS-TTS system with the augmented speaker embedding without the need for LREC. This allows the ZS-TTS system to be trained with speaker embeddings not contained in the training dataset.The training process incorporating LFCL is detailed in the following section.

 The MSP is proposed based on FastSpeech [22], a Non AutoRegressive (NAR) acoustic model with explicit duration modeling. We will introduce the model architecture, and the corresponding loss function for training. 1) Model Architecture: As shown in Fig. 6, MSP is com posed of two parts, encoder, and decoder. The text sequence t, i.e. phoneme sequence in our work, is first encoded to hid den vectors ht = Et(t), and then up-sampled by repetition according to duration sequence ˆ d = DP(ht). Notably, ˆ d is a non-negative integer sequence, where each integer means the numberhowmanyframesitscorresponding phonemelasts in x. hu is down-sampled in stages by strided convolutional layers to the corresponding stages h(1) d ,...,h(S) d . The decoder predicts P from the highest stage. First, h(S) d is fed to the decoder DS to obtain the predicted sequence ˜ p(S), which is then quantized by the corresponding codebook cS to p(S). The last hidden output sequence in the decoder and p(S) are up-sampled by repetition to concatenate with h(S−1) d as the input of the next decoder DS−1. The remaining sequences aregenerated recursively in the same way to form the predicted MSMCRP. In training, the ground truth MSMCR Z and ground-truth durations d are available in advance. Hence, we adopt teacher forcing to train MSP, i.e. replacing ˆ d with d, and inputting {z(S),...,z(2)} to decoders. 2) Loss Function: To map the input text to the expected codewords, the model is trained to estimate them directly in continuous space. Like VQ-VAE, we first purpose to approach targets by minimizing the Euclidean distance: Le = 1 S  S i=1  De p(i),z(i) (14) We also adopt the “triplet loss” [56], an effective ranking loss used in metric learning, to make the Euclidean distance between the input vector x and target codeword t ∈ c smaller than any other codewords c/t = {w|w ∈ c,w= t}. It ensures that the predicted vector can be quantized to the expected codeword. Combining with MHVQ, the “triplet loss” is written as: Dt(x,t,c)= 1 Mw∈c/t max 0, x−t 2 2 − x−w 2 2+ Lt = 1 S  S j=1  1 H  H k=1 Lj 1 Lj i=1 Dt p(j,k) i ,z(j,k) i , c(j,k) (15) where is a constant value called the margin number. In this way, the output vector will not only be closer to the target, but will also lie farther away from non-target codewords. Finally, the loss function for MSP is written as: Lmsp = Le +γ∗Lt where γ is a weight coefficient.
## EXPERIMENTAL AND RESULTS
###  3.1. Dataset and Evaluation Metric
The dataset used for training is LJSpeech[16] that is made of 13,100 audio clips with corresponding text transcripts. The training of model uses 12,588 clips while 512 clips are reserved for testing. The phoneme sequence is generated by the open-source tool g2p[14] which convert English grapheme to phoneme. Thewaveformistransformed into mel spectrogram with window and FFTlengths of 1,024, hop length of 256 and sampling rate of 22,050. The mel spectrogram has 80 channels. Montreal Force Alignment (MFA) is used to obtain the target phoneme duration. Pitch and energy ground truth values are computed using STFT and WORLD vocoder[17] respectively. Note that in order to exclude the influence of the vocoder, we uses the Griffin-Lim algorithm[18] to convert the generated mel-spectrogram into waveform audio. The DCTTS evaluation focuses on the generated speech quality. Moreover the evaluation gives priority to the number of parameters, amount of computations as measured by floating point operations (FLOPS), and sampling speed. For Mean Opinion Score (MOS) estimation, we synthesized 20 sentences from the test split with each model. The assessors were asked to estimate the quality of synthesized speech on a nine-point Likert scale, the lowest and the highest scores being 1 point and 5 points with a step of 0.5 point. The number of parameters refers to the amount of memory used in inference phase. GFLOPs reflects the number of floating point operations needed to complete an inference. GFLOPs increases with the text sequence length. In our experiment, GFLOPs is measured using 128 randomly sampled texts from the test split. The inputs to each model are same. Sampling speed is usually measured in terms of Real Time Factor(RTF is how many seconds it takes to generate one second of audio). However, The RTF leads to small fractional numbers that are less intuitive to interpret. We introduced mel spectrogram real-time-factor (mRTF) to measure the speed of DCTTS intuitively. mRTF is the number of seconds of speech divided by the mel spectrogram generation time[13].

 ###  3.2. Implementation Details
Text Encoder: The phoneme sequence x phone ∈ RN× d is an embedding of the input phonemes, where N is the sequence length and d=128 is the embedding size. The phoneme encoder is made of 2 transformer blocks. Each block is made of adepth-wise separable convolution,a Self-Attention layer and a typical transformer FFN[19] . In the FFN, we add an additional convolution layer and use the GeLU[20] activation between two linear layers. Layer Normalization is applied after Self-Attention and FFN. Both Self-Attention and FFN use residual connection for fast convergence. The acoustic features extractor consists of 2blocks. Each block includes a Convolution layer,Layer Normalization and a ReLU activation. The prediction of Energy: ye,Pitch: yp and Duration: yd are generated respectively by three extractors with same architecture. Spectrogram VQ Model: In this study, we follow VQ GAN, adopting similar network architecture for the VQ-VAE encoder Evq,decoder G, and discriminator D. To preserve more time dimension information, we set a downsampling factor of 2 a long the time axis, and a downsampling factor of 20 a long the frequency axis.For the codebook Z,the dimension of each code vectornzis set as 128, and the codebook sizeK issetas128. The learning rate is fixed and determined as a product of a base learning rate,the number of GPUs used and the batchsize. Discretecon trastive diffusion model: We built a 12 layer8-head transformer with adimension of 128 forthed if fusion model. Each transformer block contains a full-context attention,a linear fusion layer to combine conditional features and afeed-for ward network block.For the default setting, we setthetotaltimestepsT=100.The current timestept is added into the network with adaptive layer normalizati on operator. For the diffusion forward process, we linearly increase γt and βt from 0 to 0.9 and 0.1, respectively.

### 3.3. Comparison with Baseline 
We compare the results with the baseline models which were evaluated based on official implementations. Note that we set the inference steps of Grad-TTS to 100(Grad TTS-100), keeping the same inference steps as our DCTTS. Table1 shows the MOS evaluation metric as evaluated by 10 participants with high English listening comprehension. The synthesized speech samples are from the test split. Our results have great competitiveness in terms of audio quality, which indicates that our approach models the speech features effectively. Not only that,our model is also very outstanding in efficiency, it has fewer parameters and GFlOPs that are used at inference phase. The effect of the small number of parameters and GFLOPS is faster melspectrogram generation, reaching mRTF of 73.9 on a single NVIDIA1080TiGPU as shown in Table2. The speed is more evident on anIn tel CPU where DCTTS reaches mRTF of 17.6 which is 44.0× faster compared to Grad-TTS. For mel-spectrogram generation,Tacotron and Grad-TTS are unable to run with satisfactory mRTF on a single CPU